---
title: "Bayesian Modelling"
output: html_notebook
---
Load libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

library(rethinking) # the sw for model specification (it then uses cmdstan)
library(foreign) # need to load funky data format
library(here) # make sure working dir is the same all the time
set.seed(100)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Read in data

```{r}
f <- read.csv("data/a1.csv", stringsAsFactors = TRUE)

f$category <- as.numeric(f$category)
f$technique <- as.numeric(f$technique)

str(f)
```

```{r}
print(paste("Variance of all true positives:", var(f$tp)))
print(paste("Mean of all true positives: ", mean(f$tp)))

print(paste("Variance of tps using new technique"))

f_nt <- f[f$technique == 1,]
f_ot <- f[f$technique == 2,]

print(paste("Variance of new technique true positives:", var(f_nt$tp)))
print(paste("Mean of all new tehcnique true positives: ", mean(f_nt$tp)))

print(paste("Variance of old technique true positives:", var(f_ot$tp)))
print(paste("Mean of all old tehcnique true positives: ", mean(f_ot$tp)))
```

Use a predictor for technique, since we think that will better determine the tp.

```{r m_poisson_ptechnique, message=FALSE, warning=FALSE, results='hide'}
m_poisson_ptechnique <- ulam(
    alist(
        tp ~ poisson(lambda),
        log(lambda) <- a_tech[technique],
        a_tech[technique] ~ normal(0, 1)
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE
)
```
```{r}
postcheck(m_poisson_ptechnique, window = 140)
```

Prior for experience level (category).

```{r m_poisson_pexperience, message=FALSE, warning=FALSE, results='hide'}
m_poisson_pexperience <- ulam(
    alist(
        tp ~ poisson(lambda),
        log(lambda) <- a_cate[category],
        a_cate[category] ~ normal(0, 1)
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE
)
```


```{r}
postcheck(m_poisson_pexperience, window = 140)
```

#```{r m3, message=FALSE, warning=FALSE, results='hide'}
#m3 <- ulam(
#    alist(
#        tp ~ dgampois(lambda, phi),
#        logit(lambda) <- a_cate[category] + a_tech[technique],
#        a_cate[category] ~ normal(0, 10),
#        a_tech[technique] ~ normal(0, 10),
#        phi ~ normal(0, 1)
#    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE
#)
#```

#```{r}
#postcheck(m3, window = 140)
#```

```{r m_binom_ptechnique, message=FALSE, warning=FALSE, results='hide'}
m_binom_ptechnique <- ulam(
    alist(
        tp ~ binomial(25, p), #Assume TP will likely not exceed 50 
        p <- a_tech[technique],
        a_tech[technique] ~ normal(0.5, 0.1)
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE
)
```

```{r}
precis(m_binom_ptechnique)
postcheck(m_binom_ptechnique, window = 140)
```

```{r m_binom_pexperience, message=FALSE, warning=FALSE, results='hide'}
m_binom_pexperience <- ulam(
    alist(
        tp ~ binomial(25, p), #Assume TP will likely not exceed 50
        p <- a_exp[category],
        a_exp[category] ~ normal(0.5, 0.1)
    ), 
    data = f, 
    cores = 4, 
    chains = 4, 
    cmdstan = TRUE, 
    log_lik = TRUE
)
```

```{R}
precis(m_binom_pexperience)
postcheck(m_binom_pexperience, window = 140)
```



Compare models. We need to have a more refined model first...
```{r}
(loo_est <- compare(m_poisson_ptechnique, m_poisson_pexperience, m_binom_ptechnique, m_binom_pexperience, func=LOO))
```

Which experience level is better (a_cate[1] (less) or a_cate[2] (more))? It seems to be less experienced.
Which technique fares better (a_tech[1] (new) or a_tech[2] (old))? New technique is better.

```{r}
plot(precis(m_poisson_pexperience, depth = 2, pars = "a_cate", prob = 0.95))
plot(precis(m_poisson_ptechnique, depth = 2, pars = "a_tech", prob = 0.95))

plot(precis(m_binom_pexperience, depth = 2, pars = "a_exp", prob = 0.95))
plot(precis(m_binom_ptechnique, depth = 2, pars = "a_tech", prob = 0.95))
```
